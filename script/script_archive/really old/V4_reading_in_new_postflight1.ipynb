{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "####################Import all packages######################################\n",
    "#############################################################################\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "#import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import pandas_profiling as pp\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# Set working directory\n",
    "#############################################################################\n",
    "# Make Jupyter Notebook show ALL output of a cell, not only the last one\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_all_csvs_in_folder(path):\n",
    "    ''' Function reading in all CSV files in a given folder.\n",
    "        1. First lists all files in folder.\n",
    "        2. Then reads in only files which are NOT of file size = 0 and contain headers + AT LEAST 1 row of data\n",
    "        3. Appends all files into a final dataframe\n",
    "        4. Also prints the amount of files in the folder, and the amount of files used for the final dataframe.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import glob\n",
    "    import os\n",
    "    # Create list containing all files names of the current folder\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "    # These empty lists will be filled with:\n",
    "    # 1. the names of the CSV files we want to append/merge, \n",
    "    # 2. 2 lists containing numbers representing all the files that were empty (or headers but no row data) and not read\n",
    "    \n",
    "    list_with_dfs = []\n",
    "    zero_files = []\n",
    "    empty_files = []\n",
    "    empty_row_files = []\n",
    "    filename_list = []\n",
    "    n_files_appended = 0\n",
    "    \n",
    "    print('')\n",
    "    print('')\n",
    "    print('#####################################################')\n",
    "    print('STARTING READ IN')\n",
    "    print('#####################################################')\n",
    "    print('')\n",
    "    \n",
    "    for filename in all_files:        \n",
    "        print('Reading current file:')\n",
    "        print(filename)\n",
    "        # Check whether the CSV file is larger than 0Bytes (if not, it has for sure no data and will break during read in)\n",
    "        try:        \n",
    "            if os.path.getsize(filename) == 0:\n",
    "                zero_files.append(filename)                \n",
    "                print('')\n",
    "                print('------------------------------------------------------')\n",
    "                print('!!! SKIPPING FILE: ' + filename)\n",
    "                print('    REASON: FILE SIZE == 0')\n",
    "                print('------------------------------------------------------')\n",
    "                print('')\n",
    "            if os.path.getsize(filename) > 0:\n",
    "                df = pd.read_csv(filename, index_col=None, header=0)            \n",
    "                if df.empty:\n",
    "                    empty_row_files.append(filename)\n",
    "                    print('')\n",
    "                    print('------------------------------------------------------')\n",
    "                    print('!!! SKIPPING FILE: ' + filename)\n",
    "                    print('    REASON: FILE CONTAINS NO ROW DATA')\n",
    "                    print('------------------------------------------------------')\n",
    "                    print('')\n",
    "                # Check whether the CSV is empty (this ALSO works when it DOES have headers, but no row data. This is important!)\n",
    "                if not df.empty:\n",
    "                    list_with_dfs.append(df)\n",
    "                    n_files_appended = n_files_appended + 1\n",
    "                    filename_list.append(filename)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print('')\n",
    "            print('------------------------------------------------------')\n",
    "            print('!!! SKIPPING FILE: ' + filename)\n",
    "            print('    REASON: FILE ENTIRELY EMPTY WITHOUT HEADERS')\n",
    "            print('------------------------------------------------------')\n",
    "            print('')\n",
    "            empty_files.append(filename)            \n",
    "            pass\n",
    "    \n",
    "    # Merge all files in the file list\n",
    "    frame = pd.concat(list_with_dfs, axis=0, ignore_index=True)\n",
    "    print('')\n",
    "    print('#####################################################')\n",
    "    print('!!! DONE READING & MERGING DATA !!!')\n",
    "    print('#####################################################')\n",
    "    print('')\n",
    "    print('------------------------------------------------------')\n",
    "    print('All files: ' + str(len(all_files)))    \n",
    "    print('')\n",
    "    print('Successfully appended files: ' + str(n_files_appended))\n",
    "    print('------------------------------------------------------')\n",
    "    print('------------------------------------------------------')\n",
    "    print('')\n",
    "    print('Empty files without headers: ' + str(len(empty_files)))\n",
    "    #print(len(empty_files))\n",
    "    print(empty_files)\n",
    "    print('')\n",
    "    print('Zero size files: ' + str(len(zero_files)))\n",
    "    #print(len(zero_files))\n",
    "    print(zero_files)\n",
    "    print('')\n",
    "    print('Empty row files: ' + str(len(empty_row_files)))\n",
    "    #print(len(empty_row_files))\n",
    "    print(empty_row_files)\n",
    "    print('')    \n",
    "    print('#####################################################')\n",
    "    \n",
    "    return frame    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in files and merge them\n",
    "\n",
    "#PreFlight & Flight\n",
    "#path_pre_and_flight = r'../data/experiment/raw/exp_raw_pre_and_flight/' \n",
    "#pre_and_flight = read_all_csvs_in_folder(path_pre_and_flight)\n",
    "\n",
    "path_postflight1 = '/home/taco/Documents/greta_forarex/new_data/postflight_control_FINAL/Postflightdata_1/'\n",
    "#path_postflight2 = '/home/taco/Documents/greta_forarex/Neue gesammelten Daten f√ºr Timmy_08082019/Flight Experiment_Postflight_raw_20190808/Postflightdata von 15032019 bis 11042019'\n",
    "\n",
    "# PostFlight\n",
    "postflight_1 = read_all_csvs_in_folder(path_postflight1)\n",
    "#postflight_2 = read_all_csvs_in_folder(path_postflight2)\n",
    "\n",
    "# Extra Flight\n",
    "#path_extra = r'../data/experiment/raw/exp_raw_extra/2019-04-12(1)/2019-04-12/' \n",
    "#extra = read_all_csvs_in_folder(path_extra)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove all unncessary columns\n",
    "cols_to_keep = ['timeStamp',\n",
    "                'Exp0_OxygenTemp', 'Exp0_OxygenpercentO2', 'Exp0_PhValue',\n",
    "                'Exp1_OxygenTemp', 'Exp1_OxygenpercentO2',\n",
    "                'Pressure_LateAccess']\n",
    "\n",
    "#pre_and_flight = pre_and_flight[cols_to_keep]\n",
    "postflight_1 = postflight_1[cols_to_keep]\n",
    "#extra = extra[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFs contain negligible amount of missing values (NAs), just drop them\n",
    "#import missingno as msno\n",
    "#msno.matrix(postflight_1)\n",
    "#pre_and_flight.dropna(inplace=True)\n",
    "postflight_1.dropna(inplace=True)\n",
    "#extra.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive DateTime TimeStamp needs to be increased by 1hour (UTC+1 for Sweden) compared to what was recorded (apparently UTC, UK)\n",
    "#pre_and_flight.loc[:,'timeStamp'] = pd.to_datetime(pre_and_flight.timeStamp, unit='s')\n",
    "postflight_1.loc[:,'timeStamp'] = pd.to_datetime(postflight_1.timeStamp, unit='s')\n",
    "#extra.loc[:,'timeStamp'] = pd.to_datetime(extra.timeStamp, unit='s').dt.tz_localize('UTC').dt.tz_convert('Europe/Stockholm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-03-11 11:52:55')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-03-14 18:53:49')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data range of the final timestamps\n",
    "postflight_1.timeStamp.min()\n",
    "postflight_1.timeStamp.max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference between time when it was actually recorded and incorrect system time stamp\n",
    "time_delta = pd.to_datetime('2019-03-11 13:20:00') - pd.to_datetime('2000-01-01 14:30:02') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add delta to timestamp\n",
    "postflight_1.timeStamp = postflight_1.timeStamp + time_delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "postflight_1.sort_values('timeStamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>Exp0_OxygenTemp</th>\n",
       "      <th>Exp0_OxygenpercentO2</th>\n",
       "      <th>Exp0_PhValue</th>\n",
       "      <th>Exp1_OxygenTemp</th>\n",
       "      <th>Exp1_OxygenpercentO2</th>\n",
       "      <th>Pressure_LateAccess</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>2019-03-11 11:52:55</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>716.1</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15601</th>\n",
       "      <td>2019-03-11 11:53:01</td>\n",
       "      <td>14.319</td>\n",
       "      <td>9.880</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.475</td>\n",
       "      <td>5.796</td>\n",
       "      <td>716.1</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15602</th>\n",
       "      <td>2019-03-11 11:53:05</td>\n",
       "      <td>14.314</td>\n",
       "      <td>8.315</td>\n",
       "      <td>6.924</td>\n",
       "      <td>12.477</td>\n",
       "      <td>4.949</td>\n",
       "      <td>716.3</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15603</th>\n",
       "      <td>2019-03-11 11:53:11</td>\n",
       "      <td>14.314</td>\n",
       "      <td>8.334</td>\n",
       "      <td>6.924</td>\n",
       "      <td>12.485</td>\n",
       "      <td>4.940</td>\n",
       "      <td>716.6</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15604</th>\n",
       "      <td>2019-03-11 11:53:17</td>\n",
       "      <td>14.316</td>\n",
       "      <td>8.343</td>\n",
       "      <td>6.925</td>\n",
       "      <td>12.485</td>\n",
       "      <td>4.932</td>\n",
       "      <td>716.6</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15605</th>\n",
       "      <td>2019-03-11 11:53:23</td>\n",
       "      <td>14.316</td>\n",
       "      <td>8.347</td>\n",
       "      <td>6.924</td>\n",
       "      <td>12.490</td>\n",
       "      <td>4.945</td>\n",
       "      <td>717.0</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15606</th>\n",
       "      <td>2019-03-11 11:53:28</td>\n",
       "      <td>14.316</td>\n",
       "      <td>8.343</td>\n",
       "      <td>6.925</td>\n",
       "      <td>12.493</td>\n",
       "      <td>4.938</td>\n",
       "      <td>717.4</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15607</th>\n",
       "      <td>2019-03-11 11:53:35</td>\n",
       "      <td>14.309</td>\n",
       "      <td>8.352</td>\n",
       "      <td>6.925</td>\n",
       "      <td>12.498</td>\n",
       "      <td>4.945</td>\n",
       "      <td>717.9</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15608</th>\n",
       "      <td>2019-03-11 11:53:41</td>\n",
       "      <td>14.316</td>\n",
       "      <td>8.323</td>\n",
       "      <td>6.926</td>\n",
       "      <td>12.503</td>\n",
       "      <td>4.934</td>\n",
       "      <td>717.8</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15609</th>\n",
       "      <td>2019-03-11 11:53:46</td>\n",
       "      <td>14.311</td>\n",
       "      <td>8.343</td>\n",
       "      <td>6.926</td>\n",
       "      <td>12.503</td>\n",
       "      <td>4.924</td>\n",
       "      <td>718.1</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                timeStamp  Exp0_OxygenTemp  Exp0_OxygenpercentO2  Exp0_PhValue  Exp1_OxygenTemp  Exp1_OxygenpercentO2  Pressure_LateAccess  year  month  day  hour  minute  second\n",
       "15600 2019-03-11 11:52:55            0.000                 0.000         0.000            0.000                 0.000                716.1  2019      3   11    11      52      55\n",
       "15601 2019-03-11 11:53:01           14.319                 9.880         0.000           12.475                 5.796                716.1  2019      3   11    11      53       1\n",
       "15602 2019-03-11 11:53:05           14.314                 8.315         6.924           12.477                 4.949                716.3  2019      3   11    11      53       5\n",
       "15603 2019-03-11 11:53:11           14.314                 8.334         6.924           12.485                 4.940                716.6  2019      3   11    11      53      11\n",
       "15604 2019-03-11 11:53:17           14.316                 8.343         6.925           12.485                 4.932                716.6  2019      3   11    11      53      17\n",
       "15605 2019-03-11 11:53:23           14.316                 8.347         6.924           12.490                 4.945                717.0  2019      3   11    11      53      23\n",
       "15606 2019-03-11 11:53:28           14.316                 8.343         6.925           12.493                 4.938                717.4  2019      3   11    11      53      28\n",
       "15607 2019-03-11 11:53:35           14.309                 8.352         6.925           12.498                 4.945                717.9  2019      3   11    11      53      35\n",
       "15608 2019-03-11 11:53:41           14.316                 8.323         6.926           12.503                 4.934                717.8  2019      3   11    11      53      41\n",
       "15609 2019-03-11 11:53:46           14.311                 8.343         6.926           12.503                 4.924                718.1  2019      3   11    11      53      46"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postflight_1.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different time epochs for convenient plotting & exploration\n",
    "# Postflight\n",
    "postflight_1.loc[:, 'year'] = postflight_1.timeStamp.dt.year\n",
    "postflight_1.loc[:, 'month'] = postflight_1.timeStamp.dt.month\n",
    "postflight_1.loc[:, 'day'] = postflight_1.timeStamp.dt.day\n",
    "postflight_1.loc[:, 'hour'] = postflight_1.timeStamp.dt.hour\n",
    "postflight_1.loc[:, 'minute'] = postflight_1.timeStamp.dt.minute\n",
    "postflight_1.loc[:, 'second'] = postflight_1.timeStamp.dt.second\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale columns values \n",
    "# Postflight\n",
    "# All divided by 10000\n",
    "postflight_1.loc[:,'Exp0_OxygenTemp'] = postflight_1.Exp0_OxygenTemp/1000\n",
    "postflight_1.loc[:,'Exp1_OxygenTemp'] = postflight_1.Exp1_OxygenTemp/1000\n",
    "postflight_1.loc[:,'Exp0_OxygenpercentO2'] = postflight_1.Exp0_OxygenpercentO2/1000\n",
    "postflight_1.loc[:,'Exp1_OxygenpercentO2'] = postflight_1.Exp1_OxygenpercentO2/1000\n",
    "postflight_1.loc[:,'Exp0_PhValue'] = postflight_1.Exp0_PhValue/1000\n",
    "# Pressure divided by 10\n",
    "postflight_1.loc[:, 'Pressure_LateAccess'] = postflight_1.Pressure_LateAccess/10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to disk the merged files (not yet processed)\n",
    "# In Pickle format for fast read in\n",
    "#pre_and_flight.to_pickle('../data/experiment/merged/pre_and_flight_merged.pkl')\n",
    "postflight_1.to_pickle('../new_data/cleaned/postflight_1_clean.pkl')\n",
    "#extra.to_pickle('../data/experiment/merged/extra_merged.pkl')\n",
    "# In CSV\n",
    "#pre_and_flight.to_csv('../data/experiment/merged/pre_and_flight_merged.csv')\n",
    "postflight_1.to_csv('../new_data/cleaned/postflight_1_clean.csv')\n",
    "#extra.to_csv('../data/experiment/merged/extra_merged.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
